import json
import time
from collections.abc import Generator
from typing import TypedDict

from langchain_core.messages import BaseMessage, HumanMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import END, StateGraph

from config.settings import get_settings

settings = get_settings()


# -------------------- 1. ìƒíƒœ ì •ì˜ --------------------
class AgentState(TypedDict):
    task: str  # ì´ˆê¸° ê³¼ì œ
    plan: str  # Planning ì—ì´ì „íŠ¸ì˜ ê³„íšì•ˆ
    research: str  # Research ì—ì´ì „íŠ¸ì˜ ì¡°ì‚¬ ë‚´ìš©
    critique: str  # Critic ì—ì´ì „íŠ¸ì˜ ë¹„í‰
    decision: str  # Judge ì—ì´ì „íŠ¸ì˜ ìµœì¢… ê²°ì • ë˜ëŠ” ë‹¤ìŒ ë‹¨ê³„ ì§€ì‹œ
    history: list[BaseMessage]  # ì „ì²´ ëŒ€í™” ê¸°ë¡
    round_count: int  # í˜„ì¬ ë¼ìš´ë“œ ìˆ˜
    max_rounds: int  # ìµœëŒ€ ë¼ìš´ë“œ ìˆ˜


# -------------------- 2. ì¶œë ¥ í•¨ìˆ˜ ì •ì˜ --------------------
def emit_message(agent_name: str, message: str, message_type: str = "response"):
    """ì‹¤ì‹œê°„ ë©”ì‹œì§€ ì¶œë ¥ (SSE í˜•íƒœë¡œ êµ¬ì¡°í™”)"""
    output = {"agent": agent_name, "type": message_type, "content": message, "timestamp": time.time()}
    print(f"[{agent_name}] {message}")
    print(f"SSE_DATA: {json.dumps(output, ensure_ascii=False)}")
    print("-" * 50)
    time.sleep(0.5)  # ëŒ€í™” ëŠë‚Œì„ ìœ„í•œ ì•½ê°„ì˜ ì§€ì—°


# -------------------- 3. ì—ì´ì „íŠ¸ ë…¸ë“œ ì •ì˜ --------------------
# LLM ëª¨ë¸ ì„¤ì •
llm = ChatOpenAI(api_key=settings.OPENAI_API_KEY, model="gpt-3.5-turbo-1106", temperature=0.7)


def control_manager_node(state: AgentState):
    """Control Manager: í† ë¡  ì‹œì‘ ë° ê³¼ì œ ì œì‹œ"""
    emit_message("ë§¤ë‹ˆì €", f"ğŸ¯ ë¼ìš´ë“œ {state['round_count']}ì„ ì‹œì‘í•©ë‹ˆë‹¤!", "start")
    emit_message("ë§¤ë‹ˆì €", f"ì˜¤ëŠ˜ì˜ ê³¼ì œëŠ” '{state['task']}'ì…ë‹ˆë‹¤.", "task")
    emit_message("ë§¤ë‹ˆì €", "ê¸°íšìë‹˜, ê³„íšì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”.", "request")

    state["history"].append(
        HumanMessage(content=f"ë¼ìš´ë“œ {state['round_count']}: ê³¼ì œ '{state['task']}'ì— ëŒ€í•œ ë…¼ì˜ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    )
    return state


def planning_node(state: AgentState):
    """Planning: ì´ˆê¸° ê³„íš ìˆ˜ë¦½"""
    emit_message("ê¸°íšì", "ë„¤, ê³„íšì„ ìˆ˜ë¦½í•´ë³´ê² ìŠµë‹ˆë‹¤.", "thinking")

    # ì´ì „ ë¼ìš´ë“œ ì •ë³´ê°€ ìˆë‹¤ë©´ ì°¸ê³ 
    previous_context = ""
    if state["round_count"] > 1:
        previous_context = f"""
        ì´ì „ ë¼ìš´ë“œ ì •ë³´:
        - ì´ì „ ê³„íš: {state.get("plan", "N/A")}
        - ì´ì „ ë¹„í‰: {state.get("critique", "N/A")}
        ì´ì „ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ê³„íšì„ ìˆ˜ì •í•´ì£¼ì„¸ìš”.
        """

    prompt = f"""
    ë‹¹ì‹ ì€ ì „ë¬¸ ê¸°íšìì…ë‹ˆë‹¤. ë‹¤ìŒ ê³¼ì œì— ëŒ€í•´ ê°„ë‹¨í•˜ê²Œ ê³„íšì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”.
    ê³¼ì œ: {state["task"]}
    í˜„ì¬ ë¼ìš´ë“œ: {state["round_count"]}
    {previous_context}
    
    ê³„íšì€ ë‹¤ìŒì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:
    1. ëª©í‘œ ì„¤ì •
    2. ì£¼ìš” ë‹¨ê³„
    3. ì˜ˆìƒ ì¼ì •
    4. í•„ìš” ìì›
    5. ì˜ˆìƒ íš¨ê³¼
    
    ì¹œê·¼í•˜ê³  ëŒ€í™”í•˜ëŠ” í†¤ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
    """

    response = llm.invoke(prompt)
    state["plan"] = response.content
    state["history"].append(HumanMessage(content=f"[ê¸°íšì] {response.content}"))

    emit_message("ê¸°íšì", response.content, "plan")
    emit_message("ê¸°íšì", "ë¦¬ì„œì²˜ë‹˜, ì´ ê³„íšì— ëŒ€í•´ ì¡°ì‚¬í•´ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?", "request")

    return state


def research_node(state: AgentState):
    """Research: ê³„íšì— ëŒ€í•œ ì •ë³´ ì¡°ì‚¬"""
    emit_message("ë¦¬ì„œì²˜", "ê¸°íšìë‹˜ì˜ ê³„íšì„ ê²€í† í•´ë³´ê² ìŠµë‹ˆë‹¤.", "thinking")

    prompt = f"""
    ë‹¹ì‹ ì€ ì „ë¬¸ ë¦¬ì„œì²˜ì…ë‹ˆë‹¤. ê¸°íšìê°€ ì œì‹œí•œ ê³„íšì— ëŒ€í•´ ê°„ë‹¨í•˜ê²Œ ë‹¤ê°ë„ë¡œ ì¡°ì‚¬í•´ì£¼ì„¸ìš”.
    
    ê³„íšì•ˆ: {state["plan"]}
    í˜„ì¬ ë¼ìš´ë“œ: {state["round_count"]}
    
    ë‹¤ìŒ ê´€ì ì—ì„œ ì¡°ì‚¬í•´ì£¼ì„¸ìš”:
    1. ë¹„ìš©(ì‹œê°„, ë…¸ë ¥) ë¶„ì„
    2. ë¦¬ìŠ¤í¬ ë¶„ì„
    3. ì„±ê³µ ì‚¬ë¡€
    
    ì¡°ì‚¬ ê²°ê³¼ë¥¼ ëŒ€í™”í•˜ë“¯ì´ ì¹œê·¼í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    """

    response = llm.invoke(prompt)
    state["research"] = response.content
    state["history"].append(HumanMessage(content=f"[ë¦¬ì„œì²˜] {response.content}"))

    emit_message("ë¦¬ì„œì²˜", response.content, "research")
    emit_message("ë¦¬ì„œì²˜", "ë¹„í‰ê°€ë‹˜, ì´ ê³„íšê³¼ ì¡°ì‚¬ ê²°ê³¼ì— ëŒ€í•´ ì–´ë–»ê²Œ ìƒê°í•˜ì‹œë‚˜ìš”?", "request")

    return state


def critic_node(state: AgentState):
    """Critic: ê³„íšê³¼ ì¡°ì‚¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¹„íŒì  ê²€í† """
    emit_message("ë¹„í‰ê°€", "ê³„íšê³¼ ì¡°ì‚¬ ë‚´ìš©ì„ ê¼¼ê¼¼íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.", "thinking")

    prompt = f"""
    ë‹¹ì‹ ì€ ì˜ˆë¦¬í•œ ë¹„í‰ê°€ì…ë‹ˆë‹¤. ê¸°íšìì˜ ê³„íšê³¼ ë¦¬ì„œì²˜ì˜ ì¡°ì‚¬ ê²°ê³¼ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ ê²€í† í•´ì£¼ì„¸ìš”.
    
    ê³„íšì•ˆ: {state["plan"]}
    ì¡°ì‚¬ ë‚´ìš©: {state["research"]}
    í˜„ì¬ ë¼ìš´ë“œ: {state["round_count"]}
    
    ë‹¤ìŒ ê´€ì ì—ì„œ ë¹„íŒì ìœ¼ë¡œ ê²€í† í•´ì£¼ì„¸ìš”:
    1. ê³„íšì˜ í˜„ì‹¤ì„±
    2. ë…¼ë¦¬ì  ì¼ê´€ì„±
    3. ëˆ„ë½ëœ ì¤‘ìš” ìš”ì†Œ
    4. ê³¼ë„í•œ ë‚™ê´€ë¡ 
    5. êµ¬ì²´ì ì¸ ê°œì„  ì œì•ˆ
    
    ê±´ì„¤ì ì¸ ë¹„íŒì„ ëŒ€í™”í•˜ë“¯ì´ ì¹œê·¼í•˜ê²Œ ì œì‹œí•´ì£¼ì„¸ìš”.
    """

    response = llm.invoke(prompt)
    state["critique"] = response.content
    state["history"].append(HumanMessage(content=f"[ë¹„í‰ê°€] {response.content}"))

    emit_message("ë¹„í‰ê°€", response.content, "critique")
    emit_message("ë¹„í‰ê°€", "íŒì‚¬ë‹˜, ìµœì¢… ê²°ì •ì„ ë‚´ë ¤ì£¼ì„¸ìš”.", "request")

    return state


def judge_node(state: AgentState):
    """Judge: ëª¨ë“  ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •"""
    emit_message("íŒì‚¬", "ëª¨ë“  ì˜ê²¬ì„ ì¢…í•©í•˜ì—¬ íŒë‹¨í•´ë³´ê² ìŠµë‹ˆë‹¤.", "thinking")

    # ìµœëŒ€ ë¼ìš´ë“œ ì²´í¬
    is_final_round = state["round_count"] >= state["max_rounds"]

    prompt = f"""
    ë‹¹ì‹ ì€ ìµœì¢… ì˜ì‚¬ê²°ì •ê¶Œìì¸ íŒì‚¬ì…ë‹ˆë‹¤. ëª¨ë“  í† ë¡  ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ê²°ì •í•´ì£¼ì„¸ìš”.
    
    ê³„íšì•ˆ: {state["plan"]}
    ì¡°ì‚¬ ë‚´ìš©: {state["research"]}
    ë¹„í‰: {state["critique"]}
    í˜„ì¬ ë¼ìš´ë“œ: {state["round_count"]}
    ìµœëŒ€ ë¼ìš´ë“œ: {state["max_rounds"]}
    ìµœì¢… ë¼ìš´ë“œ ì—¬ë¶€: {is_final_round}
    
    {
        "ìµœì¢… ë¼ìš´ë“œì´ë¯€ë¡œ ë°˜ë“œì‹œ FINALIZEë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤."
        if is_final_round
        else "ë‹¤ìŒ ì„¸ ê°€ì§€ ì˜µì…˜ ì¤‘ í•˜ë‚˜ë¡œ ê²°ì •í•´ì£¼ì„¸ìš”:"
    }
    
    - ê³„íšì— ê·¼ë³¸ì ì¸ ìˆ˜ì •ì´ í•„ìš”í•˜ë©´ 'REVISE_PLAN'
    - ì •ë³´ê°€ ë” í•„ìš”í•˜ë©´ 'MORE_RESEARCH'  
    - ê³„íšì´ íƒ€ë‹¹í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•˜ë©´ 'FINALIZE'
    
    ê²°ì • ì´ìœ ë¥¼ ìì„¸íˆ ì„¤ëª…í•˜ê³ , ë§ˆì§€ë§‰ ì¤„ì— "DECISION: [ì„ íƒ]" í˜•ì‹ìœ¼ë¡œ ëª…ì‹œí•´ì£¼ì„¸ìš”.
    ëŒ€í™”í•˜ë“¯ì´ ì¹œê·¼í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    """

    response = llm.invoke(prompt)
    decision_text = response.content
    state["history"].append(HumanMessage(content=f"[íŒì‚¬] {decision_text}"))

    emit_message("íŒì‚¬", decision_text, "decision")

    # ê²°ì • íŒŒì‹±
    if is_final_round or "DECISION: FINALIZE" in decision_text:
        state["decision"] = "finalize"
        emit_message("íŒì‚¬", "ğŸ‰ ìµœì¢… ê²°ì •: í”„ë¡œì íŠ¸ ìŠ¹ì¸!", "final")
    elif "DECISION: REVISE_PLAN" in decision_text:
        state["decision"] = "revise_plan"
        state["round_count"] += 1
        emit_message("íŒì‚¬", f"ğŸ”„ ë¼ìš´ë“œ {state['round_count']}ì—ì„œ ê³„íšì„ ìˆ˜ì •í•˜ê² ìŠµë‹ˆë‹¤.", "continue")
    elif "DECISION: MORE_RESEARCH" in decision_text:
        state["decision"] = "more_research"
        state["round_count"] += 1
        emit_message("íŒì‚¬", f"ğŸ” ë¼ìš´ë“œ {state['round_count']}ì—ì„œ ì¶”ê°€ ì¡°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.", "continue")
    else:
        state["decision"] = "finalize"
        emit_message("íŒì‚¬", "ğŸ‰ ìµœì¢… ê²°ì •: í”„ë¡œì íŠ¸ ìŠ¹ì¸!", "final")

    return state


# -------------------- 4. ì¡°ê±´ë¶€ ì—£ì§€(ë¼ìš°í„°) ì •ì˜ --------------------
def router(state: AgentState):
    """ë¼ìš°í„°: íŒì‚¬ì˜ ê²°ì •ì— ë”°ë¼ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •"""
    decision = state.get("decision", "finalize")

    if decision == "revise_plan":
        return "planning"
    elif decision == "more_research":
        return "research"
    else:
        return END


# -------------------- 5. ê·¸ë˜í”„ êµ¬ì„± ë° ì‹¤í–‰ --------------------
def create_discussion_workflow():
    """í† ë¡  ì›Œí¬í”Œë¡œìš° ìƒì„±"""
    # ê·¸ë˜í”„ ìƒì„±
    workflow = StateGraph(AgentState)

    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("control_manager", control_manager_node)
    workflow.add_node("planning", planning_node)
    workflow.add_node("researcher", research_node)
    workflow.add_node("critic", critic_node)
    workflow.add_node("judge", judge_node)

    # ì—£ì§€ ì—°ê²°
    workflow.set_entry_point("control_manager")
    workflow.add_edge("control_manager", "planning")
    workflow.add_edge("planning", "researcher")
    workflow.add_edge("researcher", "critic")
    workflow.add_edge("critic", "judge")

    # ì¡°ê±´ë¶€ ì—£ì§€ ì—°ê²°
    workflow.add_conditional_edges("judge", router, {"planning": "planning", "research": "researcher", END: END})

    return workflow.compile()


def run_discussion(task: str, max_rounds: int = 2) -> Generator[dict, None, None]:
    """í† ë¡  ì‹¤í–‰ ë° ì‹¤ì‹œê°„ ê²°ê³¼ ë°˜í™˜"""
    app = create_discussion_workflow()

    initial_state = {
        "task": task,
        "history": [],
        "round_count": 1,
        "max_rounds": max_rounds,
        "plan": "",
        "research": "",
        "critique": "",
        "decision": "",
    }

    print("=" * 60)
    print("ğŸ¤– AI ì—ì´ì „íŠ¸ í† ë¡  ì‹œìŠ¤í…œ ì‹œì‘")
    print("=" * 60)

    # ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì‹¤í–‰í•˜ë©° ê° ë‹¨ê³„ ê²°ê³¼ ë°˜í™˜
    yield from app.stream(initial_state, stream_mode="values")

    print("=" * 60)
    print("âœ… í† ë¡  ì™„ë£Œ")
    print("=" * 60)
